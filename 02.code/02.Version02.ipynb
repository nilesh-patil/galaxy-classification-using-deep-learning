{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run = 'convolutional_21'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 00. Load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU : /gpu:1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def get_available_cpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'CPU']\n",
    "\n",
    "gpu_list = get_available_gpus()\n",
    "cpu_list = get_available_cpus()\n",
    "\n",
    "if len(gpu_list)>0:\n",
    "    device_use = str(gpu_list[-1])\n",
    "    print 'Using GPU : ' + device_use\n",
    "else:\n",
    "    device_use = str(cpu_list[-1])\n",
    "    print 'Using CPU : ' + device_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import glob as glob\n",
    "import cv2 as cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Dropout, Activation\n",
    "from keras.layers import Lambda, Conv2D, MaxPooling2D, Dense\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_train = glob.glob('../01.data/extracted/images_training_rev1/*.jpg')\n",
    "files_test = glob.glob('../01.data/extracted/images_test_rev1/*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 00. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image(file_name,size=(64,64),base_dir='../01.data/extracted/images_training_rev1/'):\n",
    "    \n",
    "    path = base_dir+str(file_name)+'.jpg'    \n",
    "    x = cv2.imread(path)\n",
    "    x = cv2.resize(x,size)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels(file_name):\n",
    "    \n",
    "    values = train_output.loc[np.int(file_name)].values\n",
    "    \n",
    "    return(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 79975, 'train': 61578}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    'train':len(files_train),\n",
    "    'test':len(files_test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_path = '../01.data/extracted/training_solutions_rev1.csv'\n",
    "train_output = pd.read_csv(y_path,index_col='GalaxyID')\n",
    "train_output.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 61578\n",
    "image_size = (424,424)\n",
    "\n",
    "\n",
    "shape_kernel = (2,2)\n",
    "shape_pool = (2,2)\n",
    "\n",
    "conv_activation = 'relu'\n",
    "dense_activation = 'relu'\n",
    "\n",
    "seed = 42\n",
    "num_classes = 37\n",
    "epochs = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 327.10it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 177.08it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 1000\n",
    "\n",
    "train_x = np.array([readImage(file_name,size=image_size)\n",
    "                  for file_name in tqdm(train_output.index[:i],\n",
    "                                        miniters=1000)\n",
    "                 ])\n",
    "\n",
    "train_y = np.array([get_labels(file_name)\n",
    "                  for file_name in tqdm(train_output.index[:i],\n",
    "                                        miniters=1000)\n",
    "                 ])\n",
    "\n",
    "train_y_expanded = np.expand_dims(np.expand_dims(train_y,1),1)\n",
    "\n",
    "samples,img_rows, img_cols,img_channels = train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_x shape:', (1000, 424, 424, 3))\n",
      "('train_y shape:', (1000, 37))\n",
      "('train_y shape:', (1000, 1, 1, 37))\n"
     ]
    }
   ],
   "source": [
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)\n",
    "print('train_y shape:', train_y_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 424, 424, 3)       0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 424, 424, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 424, 424, 3)       12        \n",
      "_________________________________________________________________\n",
      "Conv-Input-a (Conv2D)        (None, 423, 423, 32)      416       \n",
      "_________________________________________________________________\n",
      "Conv-Input-b (Conv2D)        (None, 422, 422, 32)      4128      \n",
      "_________________________________________________________________\n",
      "Conv-Input-c (Conv2D)        (None, 421, 421, 32)      4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 105, 105, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 105, 105, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 105, 105, 32)      128       \n",
      "_________________________________________________________________\n",
      "Conv-1-a (Conv2D)            (None, 104, 104, 64)      8256      \n",
      "_________________________________________________________________\n",
      "Conv-1-b (Conv2D)            (None, 103, 103, 64)      16448     \n",
      "_________________________________________________________________\n",
      "Conv-1-c (Conv2D)            (None, 102, 102, 64)      16448     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 102, 102, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 34, 34, 64)        256       \n",
      "_________________________________________________________________\n",
      "Conv-2-a (Conv2D)            (None, 33, 33, 128)       32896     \n",
      "_________________________________________________________________\n",
      "Conv-2-b (Conv2D)            (None, 32, 32, 128)       65664     \n",
      "_________________________________________________________________\n",
      "Conv-2-c (Conv2D)            (None, 31, 31, 128)       65664     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 15, 15, 128)       512       \n",
      "_________________________________________________________________\n",
      "Conv-3-a (Conv2D)            (None, 14, 14, 256)       131328    \n",
      "_________________________________________________________________\n",
      "Conv-3-b (Conv2D)            (None, 13, 13, 256)       262400    \n",
      "_________________________________________________________________\n",
      "Conv-3-c (Conv2D)            (None, 12, 12, 256)       262400    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "Conv-4-a (Conv2D)            (None, 5, 5, 512)         524800    \n",
      "_________________________________________________________________\n",
      "Conv-4-b (Conv2D)            (None, 4, 4, 512)         1049088   \n",
      "_________________________________________________________________\n",
      "Conv-4-c (Conv2D)            (None, 3, 3, 512)         1049088   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 1, 1, 1024)        525312    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1, 1, 37)          37925     \n",
      "=================================================================\n",
      "Total params: 4,060,369\n",
      "Trainable params: 4,058,379\n",
      "Non-trainable params: 1,990\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "main_input = Input(shape=(img_rows,img_cols,img_channels), name='main_input')\n",
    "\n",
    "x = Lambda(lambda x : x*1.0/255)(main_input)\n",
    "\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters=32, kernel_size=(2,2),\n",
    "           data_format='channels_last', name='Conv-Input-a' )(x)\n",
    "x = Conv2D(filters=32, kernel_size=(2,2),\n",
    "           name='Conv-Input-b')(x)\n",
    "x = Conv2D(filters=32, kernel_size=(2,2),\n",
    "           name='Conv-Input-c')(x)\n",
    "x = MaxPooling2D(pool_size=(4,4))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters=64, kernel_size=(2,2),\n",
    "           name='Conv-1-a')(x)\n",
    "x = Conv2D(filters=64, kernel_size=(2,2),\n",
    "           name='Conv-1-b' )(x)\n",
    "x = Conv2D(filters=64, kernel_size=(2,2),\n",
    "           name='Conv-1-c' )(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(3,3))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters=128, kernel_size=(2,2),\n",
    "           name='Conv-2-a' )(x)\n",
    "x = Conv2D(filters=128, kernel_size=(2,2),\n",
    "           name='Conv-2-b' )(x)\n",
    "x = Conv2D(filters=128, kernel_size=(2,2),\n",
    "           name='Conv-2-c' )(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters=256, kernel_size=(2,2),\n",
    "           name='Conv-3-a' )(x)\n",
    "x = Conv2D(filters=256, kernel_size=(2,2),\n",
    "           name='Conv-3-b' )(x)\n",
    "x = Conv2D(filters=256, kernel_size=(2,2),\n",
    "           name='Conv-3-c' )(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters=512, kernel_size=(2,2),\n",
    "           name='Conv-4-a' )(x)\n",
    "x = Conv2D(filters=512, kernel_size=(2,2),\n",
    "           name='Conv-4-b' )(x)\n",
    "x = Conv2D(filters=512, kernel_size=(2,2),\n",
    "           name='Conv-4-c' )(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024,name='features')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "classifier = Dense(37,activation='linear',name='Output')(features)\n",
    "\n",
    "model = Model(main_input,classifier,name='features')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "tb = TensorBoard(\n",
    "        log_dir='../tensorboard/'+run+'/',\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "\n",
    "mc = ModelCheckpoint(filepath = '../05.model/'+run+'.h5',\n",
    "                     save_best_only = True)\n",
    "\n",
    "ec = EarlyStopping(monitor='val_loss',\n",
    "                   patience=5,\n",
    "                   mode='auto')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.1,\n",
    "                              patience=3,\n",
    "                              min_lr=0.000000000001)\n",
    "\n",
    "tqnc = TQDMNotebookCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "\n",
    "train_steps = 2*train_x.shape[0]/batch_size\n",
    "validation_steps = 0.1 * train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "                                rotation_range=180,\n",
    "                                vertical_flip=True,\n",
    "                                horizontal_flip=True,\n",
    "                                data_format='channels_last',\n",
    "                                \n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "                                data_format='channels_last'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "                                    x=train_x,\n",
    "                                    y=train_y_expanded[:i],\n",
    "                                    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow(\n",
    "                                            x=train_x,\n",
    "                                            y=train_y_expanded[:i],\n",
    "                                            batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(device_use):\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=keras.optimizers.sgd(lr=1e-3)\n",
    "                 )\n",
    "    \n",
    "    loss_history = model.fit_generator(\n",
    "                                    generator=train_generator,\n",
    "                                    validation_data=validation_generator,\n",
    "                                    epochs=epochs,\n",
    "                                    steps_per_epoch=train_steps,\n",
    "                                    validation_steps=validation_steps,\n",
    "                                    callbacks=[tb,mc,ec,reduce_lr,tqnc],\n",
    "                                    verbose=1\n",
    "                )\n",
    "\n",
    "\n",
    "loss_df = pd.DataFrame(loss_history.history)\n",
    "loss_df.to_csv('../03.plots/losses/augmented_loss_df'+run+'.csv',\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{N} \\sum_{i=0}^{N} Actual_{i} == Predicted_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data_plot = loss_df.ix[1:,:]\n",
    "sns.set(style='whitegrid',)\n",
    "sns.plt.figure(figsize=(7.5,3.5))\n",
    "\n",
    "plt.plot(data_plot.index,data_plot.loss,label='train')\n",
    "plt.plot(data_plot.index,data_plot.val_loss,label='validation')\n",
    "plt.legend()\n",
    "plt.title('Train & Validation loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out_train = {}\n",
    "\n",
    "with tf.device(device_use):\n",
    "    \n",
    "    for file_path in tqdm(files_train):\n",
    "\n",
    "        galaxy_id = file_path.split('/')[-1].split('.')[0]\n",
    "        galaxy_img = np.expand_dims(cv2.resize(cv2.imread(file_path),\n",
    "                                               image_size),\n",
    "                                    axis=0)\n",
    "        galaxy_pred = model.predict(galaxy_img).flatten()\n",
    "\n",
    "        out_train[galaxy_id] = galaxy_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "columns = pd.read_csv(y_path,\n",
    "                      index_col='GalaxyID',\n",
    "                      nrows=0)\n",
    "\n",
    "train_results = pd.DataFrame.from_dict(data = out_train,\n",
    "                                      orient='index')\n",
    "train_results.index.name = 'GalaxyID'\n",
    "train_results.columns = columns.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/79975 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "out = {}\n",
    "\n",
    "with tf.device(device_use):  \n",
    "    for file_path in tqdm(files_test):\n",
    "        galaxy_id = file_path.split('/')[-1].split('.')[0]\n",
    "        galaxy_img = np.expand_dims(cv2.resize(cv2.imread(file_path),\n",
    "                                               image_size),\n",
    "                                    axis=0)\n",
    "        galaxy_pred = model.predict(galaxy_img).flatten()\n",
    "\n",
    "        out[galaxy_id] = galaxy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = pd.read_csv(y_path,\n",
    "                      index_col='GalaxyID',\n",
    "                      nrows=0)\n",
    "\n",
    "test_results = pd.DataFrame.from_dict(data = out,\n",
    "                                      orient='index')\n",
    "test_results.index.name = 'GalaxyID'\n",
    "test_results.columns = columns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results.to_csv('../04.results/submission'+run+'.csv',\n",
    "                    index_label='GalaxyID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
